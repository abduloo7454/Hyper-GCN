{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Libraries"
      ],
      "metadata": {
        "id": "CBAGNzWi0AJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "bPqJszPZ6Sj_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bROxrQIDxzYD"
      },
      "outputs": [],
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from scipy.stats import kendalltau\n",
        "from scipy.stats import spearmanr\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.io import loadmat\n",
        "from torch_geometric.data import Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 1/Subject12Trial1.mat')['data']\n",
        "subj1 = data1[:30,:,:]\n",
        "#subj1 = subj1.reshape(subj1.shape[2],subj1.shape[0],subj1.shape[1])\n",
        "subj2 = data1[30:,:,:]\n",
        "#subj2 = subj2.reshape(subj2.shape[2],subj2.shape[0],subj2.shape[1])\n",
        "\n",
        "data2 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 1/Subject12Trial2.mat')['data']\n",
        "subj12 = data2[:30,:,:]\n",
        "#subj12 = subj12.reshape(subj12.shape[2],subj12.shape[0],subj12.shape[1])\n",
        "subj22 = data2[30:,:,:]\n",
        "#subj22 = subj22.reshape(subj22.shape[2],subj22.shape[0],subj22.shape[1])\n",
        "\n",
        "data3 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 1/Subject34Trial1.mat')['data']\n",
        "subj13 = data3[:30,:,:]\n",
        "#subj13 = subj13.reshape(subj13.shape[2],subj13.shape[0],subj13.shape[1])\n",
        "subj23 = data3[30:,:,:]\n",
        "#subj23 = subj23.reshape(subj23.shape[2],subj23.shape[0],subj23.shape[1])\n",
        "\n",
        "data4 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 1/Subject34Trial2.mat')['data']\n",
        "subj14 = data4[:30,:,:]\n",
        "#subj14 = subj14.reshape(subj14.shape[2],subj14.shape[0],subj14.shape[1])\n",
        "subj24 = data4[30:,:,:]\n",
        "#subj24 = subj24.reshape(subj24.shape[2],subj24.shape[0],subj24.shape[1])\n",
        "\n",
        "data5  = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 2/Subject12Trial1.mat')['data']\n",
        "subj15 = data5[:30,:,:]\n",
        "#subj15 = subj15.reshape(subj15.shape[2],subj15.shape[0],subj15.shape[1])\n",
        "subj25 = data5[30:,:,:]\n",
        "#subj25 = subj25.reshape(subj25.shape[2],subj25.shape[0],subj25.shape[1])\n",
        "\n",
        "data6  = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 2/Subject12Trial2.mat')['data']\n",
        "subj16 = data6[:30,:,:]\n",
        "#subj16 = subj16.reshape(subj16.shape[2],subj16.shape[0],subj16.shape[1])\n",
        "subj26 = data6[30:,:,:]\n",
        "#subj26 = subj26.reshape(subj26.shape[2],subj26.shape[0],subj26.shape[1])\n",
        "\n",
        "data7  = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 2/Subject34Trial1.mat')['data']\n",
        "subj17 = data7[:30,:,:]\n",
        "#subj17 = subj17.reshape(subj17.shape[2],subj17.shape[0],subj17.shape[1])\n",
        "subj27 = data7[30:,:,:]\n",
        "#subj27 = subj27.reshape(subj27.shape[2],subj27.shape[0],subj27.shape[1])\n",
        "\n",
        "data8  = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 2/Subject34Trial2.mat')['data']\n",
        "subj18 = data8[:30,:,:]\n",
        "#subj18 = subj18.reshape(subj18.shape[2],subj18.shape[0],subj18.shape[1])\n",
        "subj28 = data8[30:,:,:]\n",
        "#subj28 = subj28.reshape(subj28.shape[2],subj28.shape[0],subj28.shape[1])\n",
        "\n",
        "data = np.concatenate([subj1,subj2,subj12,subj22,subj13,subj23,subj14,subj24,subj15,subj25,\n",
        "                       subj16,subj26,subj17,subj27,subj18,subj28],axis=2)\n",
        "\n",
        "data = data.reshape(data.shape[2],data.shape[0],data.shape[1])\n",
        "\n",
        "# subj1 = data[:30,:,:]\n",
        "# subj1 = subj1.reshape(subj1.shape[2],subj1.shape[0],subj1.shape[1])\n",
        "# subj2 = data[30:,:,:]\n",
        "# subj2 = subj2.reshape(subj2.shape[2],subj2.shape[0],subj2.shape[1])\n",
        "# subj = data.reshape(data.shape[2],data.shape[0],data.shape[1])\n",
        "Labels1 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 1/Subject12Trial1Label.mat')['Labels'].T   ##Labels is same for both subjects\n",
        "Labels2 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 1/Subject12Trial2Label.mat')['Labels'].T\n",
        "Labels3 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 1/Subject34Trial1Label.mat')['Labels'].T\n",
        "Labels4 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 1/Subject34Trial2Label.mat')['Labels'].T\n",
        "Labels5 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 2/Subject12Trial1Label.mat')['Labels'].T\n",
        "Labels6 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 2/Subject12Trial2Label.mat')['Labels'].T\n",
        "Labels7 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 2/Subject34Trial1Label.mat')['Labels'].T\n",
        "Labels8 = loadmat('/content/drive/MyDrive/GRAPH EEG/Experiment 2/Subject34Trial2Label.mat')['Labels'].T\n",
        "\n",
        "Labels = np.concatenate([Labels1,Labels1,Labels2,Labels2,Labels3,Labels3,Labels4,Labels4,\n",
        "                         Labels5,Labels5,Labels6,Labels6,Labels7,Labels7,Labels8,Labels8], axis=0)\n",
        "data.shape, Labels.shape"
      ],
      "metadata": {
        "id": "0b0q1a-_yA1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(Labels)"
      ],
      "metadata": {
        "id": "SgAEoFY_0NOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_trialwise_adjacency_matrices(subj1_data, subj2_data, percentile=55):\n",
        "    \"\"\"\n",
        "    Create an adjacency matrix for each trial between the channels of two subjects\n",
        "    by applying an adaptive threshold based on the percentile of correlation values.\n",
        "\n",
        "    Parameters:\n",
        "    - subj1_data (numpy.ndarray): EEG data for subject 1 of shape (trials, channels, time_points).\n",
        "    - subj2_data (numpy.ndarray): EEG data for subject 2 of shape (trials, channels, time_points).\n",
        "    - percentile (float): The percentile to use as a threshold for the adjacency matrix.\n",
        "\n",
        "    Returns:\n",
        "    - List of numpy arrays, each of shape (30, 30), representing the adjacency matrix for each trial.\n",
        "    \"\"\"\n",
        "    num_trials = subj1_data.shape[0]  # Number of trials\n",
        "    num_channels = subj1_data.shape[1]  # Number of channels\n",
        "    adjacency_matrices = []  # List to store each trial's adjacency matrix\n",
        "\n",
        "    for trial in range(num_trials):\n",
        "        # Extract the data for the current trial for each subject\n",
        "        subj1_trial = subj1_data[trial]  # Shape (30, 1000)\n",
        "        subj2_trial = subj2_data[trial]  # Shape (30, 1000)\n",
        "\n",
        "        # Initialize the correlation matrix for this trial\n",
        "        correlation_matrix = np.zeros((num_channels, num_channels))\n",
        "\n",
        "        for i in range(num_channels):\n",
        "            for j in range(num_channels):\n",
        "                # Calculate the Pearson correlation between channel i of subj1 and channel j of subj2\n",
        "                correlation = np.corrcoef(subj1_trial[i], subj2_trial[j])[0, 1]\n",
        "                correlation_matrix[i, j] = correlation\n",
        "\n",
        "        # Determine the adaptive threshold based on the specified percentile\n",
        "        threshold = np.percentile(abs(correlation_matrix), percentile)\n",
        "\n",
        "        # Convert correlation matrix to adjacency matrix based on the threshold\n",
        "        adjacency_matrix = np.where(abs(correlation_matrix) >= threshold, 1, 0)\n",
        "        # Compute degree matrix and add it to the adjacency matrix\n",
        "        # degree_matrix = np.diag(adjacency_matrix.sum(axis=1))\n",
        "        # adjacency_matrix = degree_matrix - adjacency_matrix\n",
        "\n",
        "        # Append the calculated adjacency matrix to the list\n",
        "        adjacency_matrices.append(adjacency_matrix)\n",
        "\n",
        "    return adjacency_matrices\n",
        "\n",
        "# Example usage with simulated data\n",
        "# subj1_data = np.random.rand(29, 30, 1000)  # Simulated data\n",
        "# subj2_data = np.random.rand(29, 30, 1000)  # Simulated data\n",
        "adjacency_matrices = create_trialwise_adjacency_matrices(data, data)\n",
        "print(len(adjacency_matrices))  # Should print the number of trials\n",
        "print(adjacency_matrices[0].shape)  # Should print (30, 30) for the adjacency matrix shape"
      ],
      "metadata": {
        "id": "C8oihlMY5RCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_tensor = np.stack(data)\n",
        "eeg_tensor = torch.from_numpy(eeg_tensor).clone().detach().requires_grad_(True)\n",
        "label_tensor = torch.LongTensor(Labels)\n",
        "#eeg_tensor = torch.from_numpy(s1s2).clone().detach().requires_grad_(True)"
      ],
      "metadata": {
        "id": "HZHMpCfu522G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_tensor.size(), label_tensor.size()"
      ],
      "metadata": {
        "id": "tzyOmUgPxpCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def create_pyg_graph_data(eeg_data, adjacency_matrices, labels):\n",
        "    \"\"\"\n",
        "    Convert EEG data and adjacency matrices into PyTorch Geometric graph data objects.\n",
        "\n",
        "    Parameters:\n",
        "    - eeg_data (torch.Tensor): The EEG data of shape (trials, channels, time_points).\n",
        "    - adjacency_matrices (list of np.array): List of adjacency matrices for each trial.\n",
        "    - labels (torch.Tensor): Labels for each trial, shape (trials, 1).\n",
        "\n",
        "    Returns:\n",
        "    - list of Data: A list of PyTorch Geometric Data objects, one per trial.\n",
        "    \"\"\"\n",
        "    graph_data_list = []\n",
        "\n",
        "    # Correct usage of .size() method\n",
        "    for i in range(eeg_data.size(0)):\n",
        "        # Node features: using the EEG data as is, or some transformation can be applied\n",
        "        x = eeg_data[i]  # Shape: (channels, time_points)\n",
        "\n",
        "        # Convert the adjacency matrix to edge indices\n",
        "        adj_matrix = adjacency_matrices[i]\n",
        "        edge_index = np.vstack(np.nonzero(adj_matrix)).astype(np.int64)\n",
        "\n",
        "        # Prepare the graph label\n",
        "        y = labels[i]\n",
        "\n",
        "        # Create PyTorch Geometric Data object\n",
        "        data = Data(x=torch.tensor(x, dtype=torch.float), edge_index=torch.tensor(edge_index, dtype=torch.long), y=y)\n",
        "\n",
        "        graph_data_list.append(data)\n",
        "\n",
        "    return graph_data_list\n",
        "graph_data_list = create_pyg_graph_data(eeg_tensor, adjacency_matrices, label_tensor)"
      ],
      "metadata": {
        "id": "JOJfk11Mxldh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import networkx as nx\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def visualize_graph(data, index):\n",
        "#     \"\"\" Visualize a single graph using NetworkX and matplotlib. \"\"\"\n",
        "#     G = nx.Graph()\n",
        "#     edge_index = data[index].edge_index.numpy()\n",
        "#     num_nodes = data[index].x.size(0)\n",
        "\n",
        "#     # Add nodes\n",
        "#     for node in range(num_nodes):\n",
        "#         G.add_node(node)\n",
        "\n",
        "#     # Add edges\n",
        "#     for i in range(edge_index.shape[1]):\n",
        "#         source = edge_index[0, i]\n",
        "#         target = edge_index[1, i]\n",
        "#         G.add_edge(source, target)\n",
        "\n",
        "#     # Draw the graph\n",
        "#     nx.draw(G, with_labels=True, node_color='lightblue')\n",
        "#     plt.title(f\"Graph Visualization for Trial {index}\")\n",
        "#     plt.show()\n",
        "\n",
        "# # Call this function with any of your graph data objects\n",
        "# visualize_graph(graph_data_list,index=3)"
      ],
      "metadata": {
        "id": "J5o3Ey1txrNW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch_geometric.data import DataLoader\n",
        "# import random\n",
        "\n",
        "# # Assuming graph_data_list is already defined and contains your graph data\n",
        "# random.shuffle(graph_data_list)  # Shuffle the dataset in-place\n",
        "\n",
        "# # Set the split ratios for train, validation, and test\n",
        "# train_ratio = 0.75\n",
        "# valid_ratio = 0.15\n",
        "# test_ratio = 0.10  # Ensures that train, valid, and test sum to 1\n",
        "\n",
        "# # Calculate split indices\n",
        "# train_index = int(train_ratio * len(graph_data_list))\n",
        "# valid_index = int((train_ratio + valid_ratio) * len(graph_data_list))\n",
        "\n",
        "# # Split the dataset\n",
        "# train_dataset = graph_data_list[:train_index]\n",
        "# valid_dataset = graph_data_list[train_index:valid_index]\n",
        "# test_dataset = graph_data_list[valid_index:]\n",
        "\n",
        "# # Create DataLoaders for each set\n",
        "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "# valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "import random\n",
        "\n",
        "# Assuming graph_data_list is already defined\n",
        "random.shuffle(graph_data_list)  # Shuffle dataset\n",
        "\n",
        "# Set split ratios\n",
        "train_ratio = 0.75\n",
        "valid_ratio = 0.15\n",
        "test_ratio = 1 - (train_ratio + valid_ratio)  # Ensure sum is exactly 1\n",
        "\n",
        "# Compute indices properly\n",
        "num_samples = len(graph_data_list)\n",
        "train_index = round(train_ratio * num_samples)\n",
        "valid_index = train_index + round(valid_ratio * num_samples)  # Ensure correct size\n",
        "\n",
        "# Dataset splits\n",
        "train_dataset = graph_data_list[:train_index]\n",
        "valid_dataset = graph_data_list[train_index:valid_index]\n",
        "test_dataset = graph_data_list[valid_index:]  # Remaining samples go to test\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Debugging Print\n",
        "print(f\"Dataset Sizes → Train: {len(train_dataset)}, Valid: {len(valid_dataset)}, Test: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "5iP8EIUHx4z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "\n",
        "# class GCN(torch.nn.Module):\n",
        "#     def __init__(self, num_features, num_classes):\n",
        "#         super(GCN, self).__init__()\n",
        "#         self.conv1 = GCNConv(num_features, 512)\n",
        "#         self.bn1 = BatchNorm(512)  # Batch normalization layer after first GCN layer\n",
        "#         self.conv2 = GCNConv(512, 256)\n",
        "#         self.bn2 = BatchNorm(256)  # Batch normalization layer after second GCN layer\n",
        "#         self.dropout = torch.nn.Dropout(0.5)  # Dropout layer with p=0.5\n",
        "#         self.conv3 = GCNConv(256, 128)\n",
        "#         self.bn3 = BatchNorm(128)  # Batch normalization layer after second GCN layer\n",
        "#         self.out = torch.nn.Linear(128, num_classes)\n",
        "\n",
        "#     def forward(self, data):\n",
        "#         x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "#         x = self.conv1(x, edge_index)\n",
        "#         x = self.bn1(x)\n",
        "#         x = torch.relu(x)\n",
        "#         x = self.dropout(x)\n",
        "\n",
        "#         x = self.conv2(x, edge_index)\n",
        "#         x = self.bn2(x)\n",
        "#         x = torch.relu(x)\n",
        "#         x = self.conv3(x, edge_index)\n",
        "#         x = self.bn3(x)\n",
        "#         x = torch.relu(x)\n",
        "#         x = self.dropout(x)\n",
        "\n",
        "#         x = global_mean_pool(x, batch)  # Global pooling\n",
        "#         x = self.out(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = GCN(num_features=1000, num_classes=2).to(device)  # Adjust num_classes based on your labels\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=5e-8)\n",
        "# criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "mYv_IeWM2xoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main"
      ],
      "metadata": {
        "id": "FX2TbL4MOX1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "\n",
        "class ImprovedGCN(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(ImprovedGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 512)\n",
        "        self.bn1 = BatchNorm(512)\n",
        "        self.conv2 = GCNConv(512, 256)\n",
        "        self.bn2 = BatchNorm(256)\n",
        "        self.conv3 = GCNConv(256, 128)\n",
        "        self.bn3 = BatchNorm(128)\n",
        "        self.dropout = torch.nn.Dropout(0.6)\n",
        "        self.out = torch.nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # 🛠 Fix: Aggregate node embeddings to obtain graph-level features\n",
        "        x = global_mean_pool(x, batch)  # Converts node features into graph features\n",
        "\n",
        "       # print(f\"Debug: x.shape before Linear Layer: {x.shape}\")  # Ensure shape is (batch_size, 128)\n",
        "\n",
        "        return self.out(x)  # Now passes a proper tensor to Linear layer!\n",
        "\n",
        "# Training Function\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "        total += data.y.size(0)\n",
        "\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(data.y.cpu().numpy())\n",
        "\n",
        "    return total_loss / len(train_loader), correct / total, precision_score(all_labels, all_preds, average='macro'), recall_score(all_labels, all_preds, average='macro'), f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "# Validation Function\n",
        "\n",
        "def validate(valid_loader, model, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            loss = criterion(out, data.y)\n",
        "            total_loss += loss.item()\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y).sum().item()\n",
        "            total += data.y.size(0)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.cpu().numpy())\n",
        "\n",
        "    return total_loss / len(valid_loader), correct / total, precision_score(all_labels, all_preds, average='macro'), recall_score(all_labels, all_preds, average='macro'), f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "# Test Function\n",
        "\n",
        "def test(test_loader, model, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y).sum().item()\n",
        "            total += data.y.size(0)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    return correct / total, precision_score(all_labels, all_preds, average='macro'), recall_score(all_labels, all_preds, average='macro'), f1_score(all_labels, all_preds, average='macro'), cm\n",
        "\n",
        "# Training Loop with Early Stopping and Learning Rate Scheduler\n",
        "train_accuracies, val_accuracies, test_accuracies = [], [], []\n",
        "train_precisions, val_precisions, test_precisions = [], [], []\n",
        "train_recalls, val_recalls, test_recalls = [], [], []\n",
        "train_f1s, val_f1s, test_f1s = [], [], []\n",
        "\n",
        "\n",
        "\n",
        "# Set up model, optimizer, and loss function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ImprovedGCN(num_features=1000, num_classes=2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "stopping_patience = 10\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "    train_loss, train_acc, train_prec, train_rec, train_f1 = train(train_loader, model, criterion, optimizer, device)\n",
        "    val_loss, val_acc, val_prec, val_rec, val_f1 = validate(valid_loader, model, criterion, device)\n",
        "    test_acc, test_prec, test_rec, test_f1, test_cm = test(test_loader, model, device)\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    train_precisions.append(train_prec)\n",
        "    val_precisions.append(val_prec)\n",
        "    test_precisions.append(test_prec)\n",
        "\n",
        "    train_recalls.append(train_rec)\n",
        "    val_recalls.append(val_rec)\n",
        "    test_recalls.append(test_rec)\n",
        "\n",
        "    train_f1s.append(train_f1)\n",
        "    val_f1s.append(val_f1)\n",
        "    test_f1s.append(test_f1)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    print(f'Epoch {epoch+1}: Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Print Final Metrics\n",
        "print(\"\\nFinal Performance Statistics:\")\n",
        "def print_metrics(title, metrics):\n",
        "    print(f'{title}: {np.mean(metrics):.4f} ± {np.std(metrics):.4f}')\n",
        "\n",
        "print_metrics(\"Average Train Accuracy\", train_accuracies)\n",
        "print_metrics(\"Average Validation Accuracy\", val_accuracies)\n",
        "print_metrics(\"Average Test Accuracy\", test_accuracies)\n",
        "print_metrics(\"Average Train Precision\", train_precisions)\n",
        "print_metrics(\"Average Validation Precision\", val_precisions)\n",
        "print_metrics(\"Average Test Precision\", test_precisions)\n",
        "print_metrics(\"Average Train Recall\", train_recalls)\n",
        "print_metrics(\"Average Validation Recall\", val_recalls)\n",
        "print_metrics(\"Average Test Recall\", test_recalls)\n",
        "print_metrics(\"Average Train F1 Score\", train_f1s)\n",
        "print_metrics(\"Average Validation F1 Score\", val_f1s)\n",
        "print_metrics(\"Average Test F1 Score\", test_f1s)\n"
      ],
      "metadata": {
        "id": "LI31mVgqQiVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total number of parameters"
      ],
      "metadata": {
        "id": "xOi_bmy_1B3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of parameters: {total_params}\")"
      ],
      "metadata": {
        "id": "zbgnLgbU1AcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of parameters"
      ],
      "metadata": {
        "id": "_eOKIl481G65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(model)"
      ],
      "metadata": {
        "id": "2q3UpsI31HXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_summary(model):\n",
        "  print(\"model_summary\")\n",
        "  print()\n",
        "  print(\"Layer_name\"+\"\\t\"*7+\"Number of Parameters\")\n",
        "  print(\"=\"*100)\n",
        "  model_parameters = [layer for layer in model.parameters() if layer.requires_grad]\n",
        "  layer_name = [child for child in model.children()]\n",
        "  j = 0\n",
        "  total_params = 0\n",
        "  print(\"\\t\"*10)\n",
        "  for i in layer_name:\n",
        "    print()\n",
        "    param = 0\n",
        "    try:\n",
        "      bias = (i.bias is not None)\n",
        "    except:\n",
        "      bias = False\n",
        "    if not bias:\n",
        "      param =model_parameters[j].numel()+model_parameters[j+1].numel()\n",
        "      j = j+2\n",
        "    else:\n",
        "      param =model_parameters[j].numel()\n",
        "      j = j+1\n",
        "    print(str(i)+\"\\t\"*3+str(param))\n",
        "    total_params+=param\n",
        "  print(\"=\"*100)\n",
        "  print(f\"Total Params:{total_params}\")\n",
        "\n",
        "model_summary(model)"
      ],
      "metadata": {
        "id": "DHFgB4Zn1ddY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}